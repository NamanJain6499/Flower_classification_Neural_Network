{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/flowers-recognition/flowers","execution_count":46,"outputs":[{"output_type":"stream","text":"daisy  dandelion  flowers  rose  sunflower  tulip\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n                                transforms.RandomRotation(0.2),\n                                transforms.ToTensor(),\n                                transforms.Resize((80,80))\n                               ])","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data= torchvision.datasets.ImageFolder(root=\"../input/flowers-recognition/flowers\", transform=transform)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data)","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"8646"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, val_set = torch.utils.data.random_split(data, [6917, 1729])\n\nbatch_size = 40\ntrain_loader = torch.utils.data.DataLoader(dataset=train_set,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=val_set,\n                                          batch_size=batch_size, \n                                          shuffle=True)\n","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper parameters\nnum_epochs = 15\nnum_classes = 5\nlearning_rate = 0.001\n","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(512, 1024, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc1 = nn.Linear(2*2*1024, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc1(out)\n        out = F.dropout(out, training=self.training)\n        out = self.fc2(out)\n        out = F.dropout(out, training=self.training)\n        out = self.fc3(out)\n        return F.log_softmax(out,dim=1)","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ConvNet().to(device)\n","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_step = len(train_loader)\nfor epoch in range(num_epochs):\n    training_accuracy=0\n    for i, (images, labels) in enumerate(train_loader):\n        model.train()\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Checking accuracy\n        preds = outputs.data.max(dim = 1, keepdim = True)[1]\n        training_accuracy += preds.eq(labels.data.view_as(preds)).cpu().sum()\n        \n        \n    training_accuracy = training_accuracy/len(train_loader.dataset) * 100\n        \n    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Training Accuracy: {}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item(), training_accuracy))\n","execution_count":57,"outputs":[{"output_type":"stream","text":"Epoch [1/15], Step [173/173], Loss: 1.3834, Training Accuracy: 42.70637512207031\nEpoch [2/15], Step [173/173], Loss: 1.1471, Training Accuracy: 46.711002349853516\nEpoch [3/15], Step [173/173], Loss: 1.3390, Training Accuracy: 46.985687255859375\nEpoch [4/15], Step [173/173], Loss: 1.0681, Training Accuracy: 46.87002944946289\nEpoch [5/15], Step [173/173], Loss: 1.0346, Training Accuracy: 48.171173095703125\nEpoch [6/15], Step [173/173], Loss: 1.1294, Training Accuracy: 48.14225769042969\nEpoch [7/15], Step [173/173], Loss: 1.1412, Training Accuracy: 47.202545166015625\nEpoch [8/15], Step [173/173], Loss: 1.3548, Training Accuracy: 48.229000091552734\nEpoch [9/15], Step [173/173], Loss: 1.0164, Training Accuracy: 48.95185852050781\nEpoch [10/15], Step [173/173], Loss: 1.0389, Training Accuracy: 48.156715393066406\nEpoch [11/15], Step [173/173], Loss: 1.0490, Training Accuracy: 48.214542388916016\nEpoch [12/15], Step [173/173], Loss: 1.0581, Training Accuracy: 48.24345779418945\nEpoch [13/15], Step [173/173], Loss: 1.1076, Training Accuracy: 48.30128479003906\nEpoch [14/15], Step [173/173], Loss: 1.2551, Training Accuracy: 48.18562698364258\nEpoch [15/15], Step [173/173], Loss: 1.0537, Training Accuracy: 49.197628021240234\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_step = len(train_loader)\nfor epoch in range(num_epochs):\n    testing_accuracy=0\n    for i, (images, labels) in enumerate(test_loader):\n        model.eval()\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Checking accuracy\n        preds = outputs.data.max(dim = 1, keepdim = True)[1]\n        testing_accuracy += preds.eq(labels.data.view_as(preds)).cpu().sum()\n        \n        \n    testing_accuracy = testing_accuracy/len(test_loader.dataset) * 100\n        \n    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Testing Accuracy: {}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item(), testing_accuracy))\n","execution_count":58,"outputs":[{"output_type":"stream","text":"Epoch [1/15], Step [44/173], Loss: 1.2035, Testing Accuracy: 49.045692443847656\nEpoch [2/15], Step [44/173], Loss: 1.1307, Testing Accuracy: 49.624061584472656\nEpoch [3/15], Step [44/173], Loss: 0.9670, Testing Accuracy: 49.97108459472656\nEpoch [4/15], Step [44/173], Loss: 1.1071, Testing Accuracy: 50.665122985839844\nEpoch [5/15], Step [44/173], Loss: 0.8478, Testing Accuracy: 51.87969970703125\nEpoch [6/15], Step [44/173], Loss: 1.5495, Testing Accuracy: 50.8964729309082\nEpoch [7/15], Step [44/173], Loss: 0.9214, Testing Accuracy: 51.53268051147461\nEpoch [8/15], Step [44/173], Loss: 0.9136, Testing Accuracy: 52.2845573425293\nEpoch [9/15], Step [44/173], Loss: 1.0601, Testing Accuracy: 53.267784118652344\nEpoch [10/15], Step [44/173], Loss: 0.7759, Testing Accuracy: 57.72122573852539\nEpoch [11/15], Step [44/173], Loss: 0.6092, Testing Accuracy: 56.96934509277344\nEpoch [12/15], Step [44/173], Loss: 1.0219, Testing Accuracy: 59.340660095214844\nEpoch [13/15], Step [44/173], Loss: 0.9078, Testing Accuracy: 60.266048431396484\nEpoch [14/15], Step [44/173], Loss: 0.6063, Testing Accuracy: 60.61307144165039\nEpoch [15/15], Step [44/173], Loss: 0.9613, Testing Accuracy: 62.92654800415039\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nmodel.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Train Accuracy of the model on the 8000 test images with Relu is: {} %'.format(100 * correct / total))\n","execution_count":59,"outputs":[{"output_type":"stream","text":"Train Accuracy of the model on the 8000 test images with Relu is: 42.489518577417954 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the model\nmodel.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test Accuracy of the model on the 8000 test images with Relu is: {} %'.format(100 * correct / total))\n","execution_count":60,"outputs":[{"output_type":"stream","text":"Test Accuracy of the model on the 8000 test images with Relu is: 63.67842683632157 %\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Applying Transfer Learning with vgg"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n                                transforms.RandomRotation(0.2),\n                                transforms.ToTensor(),\n                                transforms.Resize((80,80))\n                               ])","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data= torchvision.datasets.ImageFolder(root=\"../input/flowers-recognition/flowers\", transform=transform)","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, val_set = torch.utils.data.random_split(data, [6917, 1729])\n\nbatch_size = 40\ntrain_loader = torch.utils.data.DataLoader(dataset=train_set,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=val_set,\n                                          batch_size=batch_size, \n                                          shuffle=True)\n","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg = torchvision.models.vgg19(pretrained=True)\n","execution_count":64,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dde1d81af71e405d8db681bdd3470bd5"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg","execution_count":65,"outputs":[{"output_type":"execute_result","execution_count":65,"data":{"text/plain":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): ReLU(inplace=True)\n    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (24): ReLU(inplace=True)\n    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (26): ReLU(inplace=True)\n    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): ReLU(inplace=True)\n    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (33): ReLU(inplace=True)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): ReLU(inplace=True)\n    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg.classifier[6].out_features = 5\nfor param in vgg.features.parameters(): \n    param.requires_grad = False\n\nvgg = vgg.cuda()","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper parameters\nnum_epochs = 15\nnum_classes = 5\nlearning_rate = 0.001\n","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(vgg.parameters(), lr=learning_rate)","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_step = len(train_loader)\nfor epoch in range(num_epochs):\n    training_accuracy=0\n    for i, (images, labels) in enumerate(train_loader):\n        vgg.train()\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = vgg(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Checking accuracy\n        preds = outputs.data.max(dim = 1, keepdim = True)[1]\n        training_accuracy += preds.eq(labels.data.view_as(preds)).cpu().sum()\n        \n        \n    training_accuracy = training_accuracy/len(train_loader.dataset) * 100\n        \n    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Training Accuracy: {}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item(), training_accuracy))\n","execution_count":69,"outputs":[{"output_type":"stream","text":"Epoch [1/15], Step [173/173], Loss: 1.1704, Training Accuracy: 43.34248733520508\nEpoch [2/15], Step [173/173], Loss: 1.6498, Training Accuracy: 45.42431640625\nEpoch [3/15], Step [173/173], Loss: 1.7145, Training Accuracy: 44.10871887207031\nEpoch [4/15], Step [173/173], Loss: 1.5155, Training Accuracy: 45.323116302490234\nEpoch [5/15], Step [173/173], Loss: 1.0113, Training Accuracy: 46.45077133178711\nEpoch [6/15], Step [173/173], Loss: 1.0431, Training Accuracy: 45.88694763183594\nEpoch [7/15], Step [173/173], Loss: 1.1087, Training Accuracy: 46.85557174682617\nEpoch [8/15], Step [173/173], Loss: 1.4245, Training Accuracy: 47.27482986450195\nEpoch [9/15], Step [173/173], Loss: 1.3515, Training Accuracy: 46.566429138183594\nEpoch [10/15], Step [173/173], Loss: 1.2281, Training Accuracy: 47.65071487426758\nEpoch [11/15], Step [173/173], Loss: 1.0031, Training Accuracy: 47.70854568481445\nEpoch [12/15], Step [173/173], Loss: 0.9461, Training Accuracy: 46.783287048339844\nEpoch [13/15], Step [173/173], Loss: 0.9755, Training Accuracy: 48.11334228515625\nEpoch [14/15], Step [173/173], Loss: 1.0490, Training Accuracy: 48.06997299194336\nEpoch [15/15], Step [173/173], Loss: 0.9848, Training Accuracy: 47.88203048706055\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_step = len(train_loader)\nfor epoch in range(num_epochs):\n    testing_accuracy=0\n    for i, (images, labels) in enumerate(test_loader):\n        vgg.eval()\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = vgg(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Checking accuracy\n        preds = outputs.data.max(dim = 1, keepdim = True)[1]\n        testing_accuracy += preds.eq(labels.data.view_as(preds)).cpu().sum()\n        \n        \n    testing_accuracy = testing_accuracy/len(test_loader.dataset) * 100\n        \n    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Testing Accuracy: {}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item(), testing_accuracy))\n","execution_count":70,"outputs":[{"output_type":"stream","text":"Epoch [1/15], Step [44/173], Loss: 1.6417, Testing Accuracy: 49.27703857421875\nEpoch [2/15], Step [44/173], Loss: 1.5318, Testing Accuracy: 51.937538146972656\nEpoch [3/15], Step [44/173], Loss: 1.1291, Testing Accuracy: 53.7304801940918\nEpoch [4/15], Step [44/173], Loss: 0.6046, Testing Accuracy: 58.82012939453125\nEpoch [5/15], Step [44/173], Loss: 0.7793, Testing Accuracy: 64.43030548095703\nEpoch [6/15], Step [44/173], Loss: 0.5819, Testing Accuracy: 67.72701263427734\nEpoch [7/15], Step [44/173], Loss: 0.6094, Testing Accuracy: 71.02371215820312\nEpoch [8/15], Step [44/173], Loss: 0.4536, Testing Accuracy: 76.11335754394531\nEpoch [9/15], Step [44/173], Loss: 0.6674, Testing Accuracy: 77.55928039550781\nEpoch [10/15], Step [44/173], Loss: 1.0779, Testing Accuracy: 76.80740356445312\nEpoch [11/15], Step [44/173], Loss: 0.5100, Testing Accuracy: 81.08733367919922\nEpoch [12/15], Step [44/173], Loss: 0.6687, Testing Accuracy: 82.24407196044922\nEpoch [13/15], Step [44/173], Loss: 0.2389, Testing Accuracy: 82.64893341064453\nEpoch [14/15], Step [44/173], Loss: 0.3286, Testing Accuracy: 84.32620239257812\nEpoch [15/15], Step [44/173], Loss: 0.1050, Testing Accuracy: 83.80567169189453\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nvgg.train()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = vgg(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Train Accuracy of the model on the 8000 test images with Relu is: {} %'.format(100 * correct / total))\n","execution_count":71,"outputs":[{"output_type":"stream","text":"Train Accuracy of the model on the 8000 test images with Relu is: 40.29203411883765 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the model\nvgg.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = vgg(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test Accuracy of the model on the 8000 test images with Relu is: {} %'.format(100 * correct / total))\n","execution_count":72,"outputs":[{"output_type":"stream","text":"Test Accuracy of the model on the 8000 test images with Relu is: 88.37478311162522 %\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}